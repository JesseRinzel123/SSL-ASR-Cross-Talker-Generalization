{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import textgrid\n",
    "import librosa\n",
    "from numba import njit\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "processor_H = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model_H = AutoModelForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ASR model Latent Space Representation Similarity Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction HuBERT\n",
    "\n",
    "We utilize the representations from different layers of an ASR (Automatic Speech Recognition) model to abstractly represent waveform signals, aiming to simulate the paradigm of how the brain stores auditory information. Here we use HuBERT, **HuBERT** (Hidden-Unit BERT) is a ​**self-supervised speech representation model** proposed by Meta AI (formerly Facebook AI Research). It learns hierarchical representations of speech signals through pre-training on large-scale unlabeled audio data. The core innovation lies in its ​**masked prediction task**, where the model predicts acoustically meaningful units (e.g., phonemes or subword units) for randomly masked speech segments, thereby capturing both acoustic and semantic dependencies.  \n",
    "\n",
    "Key features of HuBERT include:  \n",
    "- ​**Iterative clustering**: Pseudo-labels generated through clustering refine the learning objective during training.  \n",
    "- ​**Hierarchical Transformer architecture**:  \n",
    "  - ​**Lower layers** (e.g., layers 1-6) encode ​**acoustic details** (e.g., pitch, spectral patterns)  \n",
    "  - ​**Middle layers** (7-12) capture ​**phoneme-level speech units**  \n",
    "  - ​**Higher layers** (13-24) integrate ​**semantic and syntactic information**  \n",
    "\n",
    "This layered structure aligns with hypothesized ​**hierarchical processing in the human auditory system**:  \n",
    "- Primary auditory cortex (acoustic feature extraction) →  \n",
    "- Secondary auditory regions (phoneme processing) →  \n",
    "- Association cortex (semantic integration)  \n",
    "\n",
    "By leveraging HuBERT's layer-wise representations as \"abstract acoustic templates\", we establish a computational framework to explore how the brain might store and process auditory information, bridging neuroscience hypotheses with machine learning interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HubertForCTC(\n",
      "  (hubert): HubertModel(\n",
      "    (feature_extractor): HubertFeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): HubertLayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x HubertLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x HubertLayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): HubertFeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): HubertEncoderStableLayerNorm(\n",
      "      (pos_conv_embed): HubertPositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): HubertSamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x HubertEncoderLayerStableLayerNorm(\n",
      "          (attention): HubertAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): HubertFeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (lm_head): Linear(in_features=1024, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Since the ASR we are using now is implemented in Pytorch, you can easily export our model structure by print(you_model_name)\n",
    "print(model_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HubertEncoderLayerStableLayerNorm(\n",
      "  (attention): HubertAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (feed_forward): HubertFeedForward(\n",
      "    (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
      "    (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "    (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (output_dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Use \".\" you are able to access subcomponents inside you ASR model\n",
    "# e.g. The 12th layer of transformer-layers from Encoder\n",
    "print(model_H.hubert.encoder.layers[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading audio data as Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pathset(paths,file_format=\".wav\"):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(file_format)]\n",
    "audio_dir =r\".\\data\\speech_files\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio data:  ['.\\\\data\\\\speech_files\\\\ALL_016_M_CMN_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_021_M_CMN_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_032_M_CMN_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_035_M_CMN_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_037_M_CMN_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_043_M_CMN_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_055_M_ENG_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_066_M_ENG_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_070_M_ENG_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_131_M_ENG_ENG_HT1.wav', '.\\\\data\\\\speech_files\\\\ALL_133_M_ENG_ENG_HT1.wav']\n",
      "Annotation data:  ['.\\\\data\\\\speech_files\\\\ALL_016_M_CMN_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_021_M_CMN_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_032_M_CMN_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_035_M_CMN_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_037_M_CMN_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_043_M_CMN_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_055_M_ENG_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_066_M_ENG_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_070_M_ENG_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_131_M_ENG_ENG_HT1.TextGrid', '.\\\\data\\\\speech_files\\\\ALL_133_M_ENG_ENG_HT1.TextGrid']\n",
      "\n",
      "\n",
      "sentence-level annotation:   Interval(0.042, 1.552, A BOY FELL FROM A WINDOW)\n",
      "word-level annotation:   Interval(0.153, 0.443, BOY)\n",
      "phoneme-level annotation:   Interval(0.153, 0.243, B)   Interval(0.243, 0.443, OY1)\n",
      "\n",
      "\n",
      "0.042\n",
      "1.552\n",
      "A BOY FELL FROM A WINDOW\n"
     ]
    }
   ],
   "source": [
    "# We use audio dataset from ALLSSTAR.\n",
    "# A single data record contains two files, the .wav file contains the original sound record, and the .TextGrid contains the annotation record. \n",
    "audio_path=get_pathset(audio_dir,\".wav\")\n",
    "print(\"Audio data: \",audio_path)\n",
    "textgrid_path=get_pathset(audio_dir,\".TextGrid\")\n",
    "print(\"Annotation data: \",textgrid_path)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# The annotation is performed at three levels, namely, sentence, word, and phoneme. At each level, the start, end, and mark of a single tag are recorded.\n",
    "tg = textgrid.TextGrid.fromFile(textgrid_path[0])\n",
    "print(\"sentence-level annotation:  \",tg[0][1])\n",
    "print(\"word-level annotation:  \",tg[1][3])\n",
    "print(\"phoneme-level annotation:  \",tg[2][3], \" \",tg[2][4])\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# You could read Interval() data by following method:\n",
    "Interval_sample=tg[0][1]\n",
    "print(Interval_sample.minTime)\n",
    "print(Interval_sample.maxTime)\n",
    "print(Interval_sample.mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24160,)\n"
     ]
    }
   ],
   "source": [
    "# According to this start-time and end-time with marks, we may cut some waveform we want:\n",
    "\n",
    "\n",
    "# NOTE:The HuBERT model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.\n",
    "audio, sr = librosa.load(audio_path[0])\n",
    "target_sr=16000\n",
    "wave_res = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "tg_sentence = tg[0][1]\n",
    "\n",
    "\n",
    "start_sentence = int(tg_sentence.minTime*target_sr) \n",
    "end_sentence = int(tg_sentence.maxTime*target_sr)\n",
    "waveform_input=wave_res[start_sentence:end_sentence]\n",
    "print(waveform_input.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3 Get the feature output from different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature output from 1-CNN-Layer:  torch.Size([1, 4831, 512])\n",
      "Feature output from 2-CNN-Layer:  torch.Size([1, 2415, 512])\n",
      "Feature output from 3-CNN-Layer:  torch.Size([1, 1207, 512])\n",
      "Feature output from 4-CNN-Layer:  torch.Size([1, 603, 512])\n",
      "Feature output from 5-CNN-Layer:  torch.Size([1, 301, 512])\n",
      "Feature output from 6-CNN-Layer:  torch.Size([1, 150, 512])\n",
      "Feature output from 7-CNN-Layer:  torch.Size([1, 75, 512])\n"
     ]
    }
   ],
   "source": [
    "# Since HuBERT model has a fixed torch structure, we need also normalize input format:\n",
    "input=processor_H(wave_res[start_sentence:end_sentence], sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "\n",
    "# First, we could get result from CNN-layers by simply feature extractor functions\n",
    "model_H.eval() \n",
    "conv_outputs = []\n",
    "x = input.clone() # avoid change the original data\n",
    "for conv_layer in model_H.hubert.feature_extractor.conv_layers:\n",
    "    x = conv_layer(x)\n",
    "    conv_outputs.append(x.unsqueeze(0).transpose(2,1))\n",
    "    \n",
    "for _,i in enumerate(conv_outputs):\n",
    "    print(f\"Feature output from {_+1}-CNN-Layer: \",i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature output from 0-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 1-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 2-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 3-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 4-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 5-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 6-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 7-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 8-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 9-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 10-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 11-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 12-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 13-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 14-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 15-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 16-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 17-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 18-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 19-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 20-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 21-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 22-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 23-Transformers-Layer:  torch.Size([1, 75, 1024])\n",
      "Feature output from 24-Transformers-Layer:  torch.Size([1, 75, 1024])\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "with torch.no_grad():\n",
    "    outputs = model_H(input, output_hidden_states=True)\n",
    "    all_layer_outputs = outputs.hidden_states\n",
    "for _,i in enumerate(all_layer_outputs):\n",
    "    print(f\"Feature output from {_+0}-Transformers-Layer: \",i.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get the prediction of audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton from HuBERT:  A BOY FELL FROM THE WINDOW\n",
      "Actual Result:  A BOY FELL FROM A WINDOW\n",
      "Prediction Correct!\n"
     ]
    }
   ],
   "source": [
    "input=processor_H(wave_res[start_sentence:end_sentence], sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "model_H.eval() \n",
    "with torch.no_grad():\n",
    "    output_encoder = model_H(input).logits\n",
    "outind=torch.argmax(output_encoder,dim=-1).cpu().numpy()\n",
    "transcription = processor_H.batch_decode(outind)[0]\n",
    "print(\"Prediciton from HuBERT: \", transcription)\n",
    "print(\"Actual Result: \", tg_sentence.mark)\n",
    "print(\"Prediction Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
