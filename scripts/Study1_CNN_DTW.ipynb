{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from phonecodes import phonecodes\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import tqdm\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textgrid\n",
    "from scipy.spatial.distance import euclidean\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import nltk\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "import soundfile as sf\n",
    "import pickle\n",
    "_ESPEAK_LIBRARY = r\"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n",
    "'''processor_P = AutoProcessor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model_P = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")'''\n",
    "\n",
    "# loading Hubert model\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "processor_H = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model_H = AutoModelForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create 3d t-SNE latent speech representation from Hubert CNN last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading xls file dataframe from xie2021. We only use the experiment 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_result_path=r\"..\\data\\test.xlsx\"\n",
    "human_result = pd.read_excel(human_result_path)\n",
    "human_result_1a=human_result[human_result[\"Experiment\"]==\"1a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the audio data.  \n",
    "note: we use different set for exposure and test, they all from ALLSTAR Hint1 set. Each set got 16 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathset(paths):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(\".wav\")]\n",
    "audio_dir =r\"..\\data\\speech_files\"\n",
    "set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate latent space representation with Hubert-CNN last layer output. Here we generate one sentence's representation for each time. Since too short or too long audio data, such as at the word-level or entire paragraphs, may lead to different transcription mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "def build_tSNE_matrices(audio_dir, set_list, model, processor):\n",
    "    audio_path=get_pathset(audio_dir)[::-1]\n",
    "    sentences=[[] for i in range(32)]\n",
    "    \n",
    "    #get each talker's audio data\n",
    "    for each_path in audio_path:\n",
    "        audio, sr = librosa.load(each_path)\n",
    "        wave_res = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "        tg = textgrid.TextGrid.fromFile(each_path[:-3]+\"TextGrid\")\n",
    "        tg_sentence = tg[0]\n",
    "        for _,i in enumerate(tg[0]):\n",
    "            if i.mark!=\"\":\n",
    "                tg_sentence[_-1].maxTime=tg_sentence[_].minTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.mark!=\"\"]\n",
    "        tg_sentence=[tg_sentence[i] for i in set_list]\n",
    "        \n",
    "        # feed each sentence's audio data into Hubert\n",
    "        for _, each_sentence in enumerate(tg_sentence):\n",
    "            start_sentence = int(each_sentence.minTime*16000)\n",
    "            end_sentence = int(each_sentence.maxTime*16000)\n",
    "            input=processor(wave_res[start_sentence:end_sentence], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder=model.hubert.feature_extractor(input).transpose(2,1).cpu().numpy()\n",
    "            sentences[_].append(out_encoder) # store by sentence level.\n",
    "    \n",
    "    # flatten data then apply 3d t-SNE.\n",
    "    flatten=np.array([x for i in sentences for j in i for x in j[0]])\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    reduced_data = tsne.fit_transform(flatten)\n",
    "    mean = np.mean(reduced_data, axis=0)\n",
    "    std = np.std(reduced_data, axis=0)\n",
    "    reduced_data= (reduced_data - mean) / std\n",
    "    count=0\n",
    "    \n",
    "    for _,i in enumerate(sentences):\n",
    "        for __,j in enumerate(i):\n",
    "            sentences[_][__]=reduced_data[count:count+j.shape[1]]\n",
    "            count+=j.shape[1]\n",
    "    return sentences\n",
    "\n",
    "set_list=set1_list+set2_list\n",
    "sentence_matrix = build_tSNE_matrices(audio_dir, set_list, model_H, processor_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the 3d t-SNE data for later usage.\n",
    "\n",
    "\n",
    "#with open(\"..\\data\\hubert_sentences_CNN.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(sentence_matrix, file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the 3d t-SNE data\n",
    "\n",
    "#with open(\"..\\data\\hubert_sentences.pkl\", \"rb\") as file:\n",
    "#    sentence_matrix1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply DTW on word-level to calculate the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_dict(human_result_1a):\n",
    "    '''\n",
    "    Return a dict, key: sentenceID, values: keywords\n",
    "    '''\n",
    "    keywords_dict={}\n",
    "    for each_ in human_result_1a.values:\n",
    "        sentenceID=each_[human_result_1a.columns.get_loc(\"SentenceID\")]\n",
    "        if sentenceID not in keywords_dict:\n",
    "            keywords_dict[sentenceID]=[]\n",
    "        keyword=each_[human_result_1a.columns.get_loc(\"Keyword\")]\n",
    "        if keyword not in keywords_dict[sentenceID]:\n",
    "            keywords_dict[sentenceID].append(keyword)\n",
    "    return dict(sorted(keywords_dict.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a word level 3d feature set.\n",
    "\n",
    "def create_set(audio_dir, df, reduced_data):\n",
    "    set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "    set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "    keywords_dict=get_keywords_dict(df)\n",
    "    keywords=[j for i in list(keywords_dict.values()) for j in i]\n",
    "    audio_path=get_pathset(audio_dir)[::-1]\n",
    "    out_dict={}\n",
    "    \n",
    "    word_features=[[] for i in range(len(keywords))]\n",
    "    for __, each_path in enumerate(audio_path):\n",
    "        \n",
    "        current_talker=os.path.basename(each_path)[:13]\n",
    "        if current_talker not in out_dict.keys():\n",
    "            out_dict[current_talker]=[[] for i in range(32)]\n",
    "        tg = textgrid.TextGrid.fromFile(each_path[:-3]+\"TextGrid\")\n",
    "        tg_sentence = tg[0]\n",
    "        for _,i in enumerate(tg[0]):\n",
    "            if i.mark!=\"\":\n",
    "                tg_sentence[_-1].maxTime=tg_sentence[_].minTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.mark!=\"\"]\n",
    "        tg_sentence=[tg_sentence[i] for i in set1_list+set2_list]\n",
    "        tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "        \n",
    "        count=0\n",
    "        for _,each_sentence in enumerate(tg_sentence):\n",
    "            sentence_total_length=each_sentence.maxTime-each_sentence.minTime\n",
    "            \n",
    "            for key_word in list(keywords_dict.values())[_]:\n",
    "                \n",
    "                for each_word_tg in tg_word:\n",
    "                    if each_word_tg.mark.lower()==key_word:\n",
    "                        if each_word_tg.minTime >= each_sentence.minTime and each_word_tg.maxTime <= each_sentence.maxTime:                        \n",
    "                            start=each_word_tg.minTime\n",
    "                            end=each_word_tg.maxTime\n",
    "                            break\n",
    "\n",
    "                word_cut_start=start-each_sentence.minTime\n",
    "                word_cut_end=end-each_sentence.minTime\n",
    "                word_start=round(reduced_data[_][__].shape[0]*word_cut_start/sentence_total_length)\n",
    "                word_end=round(reduced_data[_][__].shape[0]*word_cut_end/sentence_total_length)\n",
    "                \n",
    "                features=copy.deepcopy(reduced_data[_][__][word_start:word_end,:])\n",
    "                word_features[count].append(features)\n",
    "                out_dict[current_talker][_].append(features)\n",
    "                count+=1\n",
    "\n",
    "    return word_features,out_dict\n",
    "\n",
    "\n",
    "word_features,out_dict=create_set(audio_dir, human_result_1a, sentence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_paths(TrainingTalkerID):\n",
    "    path_list=[]\n",
    "    TalkerID=[]\n",
    "    for each_ID in TrainingTalkerID.split(\", \"):\n",
    "        if each_ID[:3]==\"CMN\":\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_CMN\")\n",
    "        else:\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_ENG\")\n",
    "    return TalkerID\n",
    "\n",
    "def get_keywords_list(df):\n",
    "    out_dict={}\n",
    "    for each_ in human_result_1a.values:\n",
    "        keyword_loc=df.columns.get_loc(\"Keyword\")\n",
    "        key_word = each_[keyword_loc]\n",
    "        sentenceID = each_[df.columns.get_loc(\"SentenceID\")]\n",
    "        if sentenceID not in out_dict.keys():\n",
    "            out_dict[sentenceID]=[]\n",
    "        if key_word not in out_dict[sentenceID]:\n",
    "            out_dict[sentenceID].append(key_word)\n",
    "    return out_dict\n",
    "\n",
    "def get_exposure_set(feature_dict,trainingTalkerID,sentenceID,key_word):\n",
    "    keywors_list = get_keywords_list(human_result_1a)\n",
    "    set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "    set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "    features=[]\n",
    "    for i in trainingTalkerID:\n",
    "        sentence_ind=(set1_list+set2_list).index(int(sentenceID[-3:])-1)\n",
    "        key_word_ind=keywors_list[sentenceID].index(key_word)\n",
    "        features.append(copy.deepcopy(feature_dict[i][sentence_ind][key_word_ind]))\n",
    "    return features\n",
    "\n",
    "def get_test_feature(feature_dict, test_talker, sentenceID, key_word):\n",
    "    keywors_list = get_keywords_list(human_result_1a)\n",
    "    set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "    set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "    #print(sentenceID)\n",
    "    sentence_ind=(set1_list+set2_list).index(int(sentenceID[-3:])-1)\n",
    "    key_word_ind=keywors_list[sentenceID].index(key_word)\n",
    "    \n",
    "    return copy.deepcopy(feature_dict[test_talker][sentence_ind][key_word_ind])\n",
    "\n",
    "\n",
    "def sim_measure1(df, feature_dict):\n",
    "    sim_min_list=[]\n",
    "    sim_std_list=[]\n",
    "    sim_median_list=[]\n",
    "    sim_mean_list=[]\n",
    "    \n",
    "    train_set_dict={}\n",
    "    test_word_dict={}\n",
    "\n",
    "    train_talker_list=[]\n",
    "    test_talker_list=[]\n",
    "    out_df=pd.DataFrame(columns=['Condition2', 'TrainingTalkerID', 'TestTalkerID','SentenceID', 'Keyword',  'distance_min', 'IsCorrect'])#'trial',\n",
    "    for each_ in tqdm.tqdm(df.values):\n",
    "        filename_loc=df.columns.get_loc(\"Filename\")\n",
    "        keyword_loc=df.columns.get_loc(\"Keyword\")\n",
    "        sentence_loc= df.columns.get_loc(\"SentenceID\")\n",
    "        training_talker_loc=df.columns.get_loc(\"TrainingTalkerID\")\n",
    "        \n",
    "        set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "        set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "        if each_[df.columns.get_loc(\"TrainingTestSet\")] == \"set2,set1\":\n",
    "            train_set=set2_list\n",
    "            test_set=set1_list\n",
    "        else:\n",
    "            train_set=set1_list\n",
    "            test_set=set2_list\n",
    "        \n",
    "        test_file = [os.path.basename(each_[df.columns.get_loc(\"Filename\")])[:13]]\n",
    "        key_word = each_[keyword_loc] #string\n",
    "        TrainingTalkerID = each_[training_talker_loc] #list of string\n",
    "        sentenceID = each_[df.columns.get_loc(\"SentenceID\")]\n",
    "        trainingTalkerID=get_training_paths(TrainingTalkerID)\n",
    "        \n",
    "        train_talker_key=\",\".join(sorted(trainingTalkerID))\n",
    "        if train_talker_key not in train_set_dict:\n",
    "            train_set_dict[train_talker_key]={}\n",
    "        if sentenceID not in train_set_dict[train_talker_key]:\n",
    "            train_set_dict[train_talker_key][sentenceID]={}\n",
    "        if key_word not in train_set_dict[train_talker_key][sentenceID]:\n",
    "            \n",
    "            training_features=get_exposure_set(feature_dict,trainingTalkerID,sentenceID,key_word)\n",
    "            train_set_dict[train_talker_key][sentenceID][key_word]=copy.deepcopy(training_features)\n",
    "        else:\n",
    "            training_features=train_set_dict[train_talker_key][sentenceID][key_word]\n",
    "            \n",
    "        #training_features=get_exposure_set(feature_dict,trainingTalkerID,sentenceID,key_word)\n",
    "        \n",
    "        \n",
    "        if test_file[0] not in test_word_dict:\n",
    "            test_word_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_word_dict[test_file[0]]:\n",
    "            test_word_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_word_dict[test_file[0]][sentenceID]:\n",
    "            test_feature = get_test_feature(feature_dict, test_file[0], sentenceID, key_word)\n",
    "            test_word_dict[test_file[0]][sentenceID][key_word]=copy.deepcopy(test_feature)\n",
    "        else:\n",
    "            test_feature = test_word_dict[test_file[0]][sentenceID][key_word]\n",
    "        \n",
    "        train_talker_list.append(\",\".join(trainingTalkerID))\n",
    "        test_talker_list.append(test_file[0])\n",
    "        sims=[]\n",
    "        \n",
    "        for _, each_train_feature in enumerate(training_features):\n",
    "            #print(each_train_feature)\n",
    "            #print(test_feature)\n",
    "            X=each_train_feature.transpose()\n",
    "            Y=test_feature.transpose()\n",
    "            D, wp = librosa.sequence.dtw(X, Y, metric='euclidean')\n",
    "            \n",
    "            \n",
    "            \n",
    "            combined_length = X.shape[1] + Y.shape[1]\n",
    "            normalized_distance = D[-1, -1] / combined_length\n",
    "\n",
    "            sims.append(normalized_distance)\n",
    "            \n",
    "            #out_df=pd.DataFrame(columns=['Condition2', 'TrainingTalkerID', 'TestTalkerID', 'Keyword','distance_min', 'IsCorrect'])\n",
    "        out_df.loc[len(out_df)]=[each_[df.columns.get_loc(\"Condition2\")], each_[df.columns.get_loc(\"TrainingTalkerID\")],#trainingTalkerID[_],\n",
    "                                    each_[df.columns.get_loc(\"TestTalkerID\")], each_[sentence_loc],\n",
    "                                    each_[df.columns.get_loc(\"Keyword\")], \n",
    "                                    #each_[df.columns.get_loc('trial')],\n",
    "                                    sims, \n",
    "                                    each_[df.columns.get_loc(\"IsCorrect\")]]\n",
    "\n",
    "        \n",
    "        #sim_min=np.min(sims)\n",
    "        #sim_std=np.std(sims)\n",
    "        #sim_median=np.median(sims)\n",
    "        #sim_mean=np.mean(sims)\n",
    "        #\n",
    "        #sim_min_list.append(sim_min)\n",
    "        #sim_std_list.append(sim_std)\n",
    "        #sim_median_list.append(sim_median)\n",
    "        #sim_mean_list.append(sim_mean)\n",
    "\n",
    "        \n",
    "    return out_df\n",
    "\n",
    "\n",
    "\n",
    "out_df=sim_measure1(human_result_1a, out_dict)\n",
    "new_df=copy.deepcopy(out_df)\n",
    "#convert distance into similarity\n",
    "new_df['similarity'] = np.exp(-1*new_df['distance_min'].apply(lambda x:np.min(x)))\n",
    "\n",
    "#groupby 3296 unique combination\n",
    "TTID=new_df.columns.get_loc(\"TrainingTalkerID\")\n",
    "new_TrainingTalkerID=[]\n",
    "for i in new_df.values:\n",
    "    new_TrainingTalkerID.append( \",\".join(sorted(i[TTID].split(\", \"))) )\n",
    "new_df[\"TrainingTalkerID1\"]=new_TrainingTalkerID\n",
    "new_df['trial'] = (new_df.groupby(['Keyword', 'Condition2', 'TrainingTalkerID1', 'TestTalkerID', 'SentenceID'], as_index=False).ngroup() + 1)\n",
    "\n",
    "\n",
    "new_df2=new_df.groupby(['Keyword', 'Condition2', 'TrainingTalkerID1', 'TestTalkerID', 'SentenceID','trial'], as_index=False).agg( #'sentenceID'\n",
    "    IsCorrect=('IsCorrect', 'mean'),\n",
    "    #distance=('distance_min', 'min'),\n",
    "    similarity=('similarity', 'mean'),\n",
    "    numCorrect=('IsCorrect', lambda x: (x==1).sum()),\n",
    "    numIncorrect=('IsCorrect', lambda x: (x==0).sum())\n",
    "    )\n",
    "new_df2.to_csv('..\\data\\df_CNN_k=1_max.csv', index=False)\n",
    "#new_df2.to_csv('new_df2m.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
