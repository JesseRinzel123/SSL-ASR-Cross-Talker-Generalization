{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/hubert-large-ls960-ft were not used when initializing HubertForCTC: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-large-ls960-ft and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from phonecodes import phonecodes\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import tqdm\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textgrid\n",
    "from scipy.spatial.distance import euclidean\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "import soundfile as sf\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "processor_H = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n",
    "model_H = AutoModelForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR_HOME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mProgram Files\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mR\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mR-4.4.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpathsep \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProgram Files\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mR-4.4.1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mx64\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrpy2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobjects\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpackages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m importr\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['R_HOME'] = 'C:\\\\Program Files\\\\R\\\\R-4.4.1'\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\R\\R-4.4.1\\bin\\x64\"\n",
    "\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import Formula, pandas2ri\n",
    "\n",
    "import pandas as pd\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathset(paths):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(\".wav\")]\n",
    "audio_dir =r\"..\\Feb09\\audio_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_PATH = \"../Feb_15/LTASmatched_noise.wav\"\n",
    "DEFAULT_SNR_LEVELS = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "MAX_GAIN = 9\n",
    "\n",
    "def get_group_id(index, group_size=15):\n",
    "    return index // group_size\n",
    "\n",
    "\n",
    "def safe_normalize(audio, max_peak=0.99):\n",
    "    peak = np.max(np.abs(audio))\n",
    "    if peak > max_peak:\n",
    "        return audio * (max_peak / peak)\n",
    "    return audio\n",
    "\n",
    "def extend_noise(noise, target):\n",
    "    while len(target)>len(noise):\n",
    "        noise=np.append(noise,noise)\n",
    "    return noise\n",
    "\n",
    "def add_noise_v2(sentence_speech, sentence_index, \n",
    "                noise_path=NOISE_PATH, \n",
    "                snr_levels=DEFAULT_SNR_LEVELS,\n",
    "                max_gain=MAX_GAIN):\n",
    "    \n",
    "    if len(sentence_speech) == 0:\n",
    "        raise ValueError(\"Input speech cannot be empty\")\n",
    "    \n",
    "    try:\n",
    "        noise, sr = librosa.load(noise_path, sr=None)\n",
    "        noise = librosa.resample(noise, orig_sr=sr, target_sr=16000)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load noise file: {e}\")\n",
    "    \n",
    "    group = get_group_id(sentence_index)\n",
    "    if group >= len(snr_levels):\n",
    "        return sentence_speech\n",
    "    target_SNR = snr_levels[group]\n",
    "    \n",
    "    if target_SNR > max_gain:\n",
    "        return sentence_speech\n",
    "    \n",
    "    speech_RMS = np.sqrt(np.mean(sentence_speech**2))\n",
    "    noise_RMS = np.sqrt(np.mean(noise**2))\n",
    "    scale_factor = (speech_RMS / (10 ** (target_SNR / 20))) / noise_RMS\n",
    "    \n",
    "    scaled_noise = noise * scale_factor\n",
    "    extended_noise = extend_noise(scaled_noise, sentence_speech)\n",
    "    mixed = sentence_speech + extended_noise[:len(sentence_speech)]\n",
    "\n",
    "    mixed = safe_normalize(mixed)\n",
    "    return np.clip(mixed, -1.0, 1.0)\n",
    "\n",
    "def build_tSNE_matrices(audio_dir, model, processor):\n",
    "    audio_path=get_pathset(audio_dir)\n",
    "    sentences=[[] for i in range(60)]\n",
    "    #print(len(sentences))\n",
    "    for each_path in tqdm.tqdm(audio_path):\n",
    "        audio, sr = librosa.load(each_path)\n",
    "        wave_res = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "        tg = textgrid.TextGrid.fromFile(each_path[:-3]+\"TextGrid\")\n",
    "        tg_sentence = tg[0]\n",
    "        for _,i in enumerate(tg[0]):\n",
    "            if i.mark!=\"\":\n",
    "                tg_sentence[_-1].maxTime=tg_sentence[_].minTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.mark!=\"\"]\n",
    "        if \"HT1\" in each_path:\n",
    "            set_list=get_sentence_ind(audio_path[0],human_result)\n",
    "            #print(\"set hint1: \",len(set_list))\n",
    "        else:\n",
    "            set_list=get_sentence_ind(audio_path[1],human_result)\n",
    "            #print(\"set hint2: \",len(set_list))\n",
    "        tg_sentence=[tg_sentence[i] for i in set_list]\n",
    "        #print(\"Now: \",len(tg_sentence))\n",
    "        \n",
    "        for _, each_sentence in enumerate(tg_sentence):\n",
    "            \n",
    "            start_sentence = int(each_sentence.minTime*16000)\n",
    "            end_sentence = int(each_sentence.maxTime*16000)\n",
    "            mixed_audio=add_noise_v2(wave_res[start_sentence:end_sentence], _)\n",
    "            #wave_res[start_sentence:end_sentence]\n",
    "            input=processor(mixed_audio, sampling_rate=16000, return_tensors=\"pt\").input_values#.to(device)\n",
    "            #model.to(device)\n",
    "            model.eval() \n",
    "            #out_encoder = []\n",
    "            conv_outputs = []\n",
    "            x = input.clone()  # 克隆输入以避免修改原始数据\n",
    "\n",
    "            # 逐层提取特征并保存\n",
    "            for conv_layer in model_H.hubert.feature_extractor.conv_layers:\n",
    "                x = conv_layer(x)\n",
    "                conv_outputs.append(x.unsqueeze(0).transpose(2,1))\n",
    "            #with torch.no_grad():\n",
    "            #    outputs = model(input, output_hidden_states=True)\n",
    "            #    all_layer_outputs = outputs.hidden_states\n",
    "                #out_encoder=model.hubert.feature_extractor(input).transpose(2,1).cpu().numpy()\n",
    "                #out_encoder=model.hubert(input)[0].cpu().numpy()\n",
    "                #out_encoder=model_H.hubert.feature_extractor.conv_layers[4](input).unsqueeze(0).transpose(2,1).cpu().numpy()\n",
    "                #out_encoder=model_H.hubert.feature_projection(model_H.hubert.feature_extractor(input).transpose(2,1)[0]).cpu().numpy()\n",
    "            #out_encoder=all_layer_outputs[12].cpu().numpy()\n",
    "            out_encoder=conv_outputs[0].detach().numpy()\n",
    "            if \"HT1\" in each_path:\n",
    "                sentences[_].append(out_encoder)\n",
    "            else:\n",
    "                sentences[_+31].append(out_encoder)\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    flatten=np.array([x for i in sentences for j in i for x in j[0]])\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    reduced_data = tsne.fit_transform(flatten)\n",
    "    mean = np.mean(reduced_data, axis=0)\n",
    "    std = np.std(reduced_data, axis=0)\n",
    "    reduced_data= (reduced_data - mean) / std\n",
    "    count=0\n",
    "    \n",
    "    for _,i in enumerate(sentences):\n",
    "        for __,j in enumerate(i):\n",
    "            sentences[_][__]=reduced_data[count:count+j.shape[1]]\n",
    "            count+=j.shape[1]\n",
    "    return sentences\n",
    "def get_sentence_ind(audio_path,human_result):\n",
    "    tg = textgrid.TextGrid.fromFile(audio_path[:-3]+\"TextGrid\")\n",
    "    tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "    hint1=[]\n",
    "    for _,i in enumerate(tg_sentence):\n",
    "        if i.mark in set(human_result['sentence_test']):\n",
    "            hint1.append(_)\n",
    "    return hint1\n",
    "\n",
    "human_result_path=r\"..\\Feb09\\BBP-2023-TestData1.xlsx\"\n",
    "human_result = pd.read_excel(human_result_path)\n",
    "human_result=human_result[human_result[\"training_condition\"]!=\"a_control\"]\n",
    "#sentence_matrix = build_tSNE_matrices(audio_dir, model_H, processor_H)\n",
    "\n",
    "#with open(\"hubert_T12.pkl\", \"wb\") as f:\n",
    "#    pickle.dump(sentence_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hubert_T24.pkl\", \"rb\") as file:\n",
    "    sentence_matrix1 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "@njit\n",
    "def weighted_minkowski(vec1, vec2,  tau, w=1):\n",
    "\n",
    "    total = 0.0\n",
    "    for m in range(len(vec1)):\n",
    "        diff = w*abs(vec1[m] - vec2[m])\n",
    "        total += (diff ** tau)\n",
    "    return total**(1/tau)#np.sqrt(total)\n",
    "\n",
    "@njit\n",
    "def dtw_sim(seq1, seq2,  tau, k):\n",
    "    n, m = len(seq1), len(seq2)\n",
    "    dtw_matrix = np.full((n+1, m+1), np.inf)\n",
    "    dtw_matrix[0, 0] = 0.0\n",
    "\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost = weighted_minkowski(seq1[i-1], seq2[j-1], tau)\n",
    "            dtw_matrix[i, j] = cost + min(dtw_matrix[i-1, j],    # insertion\n",
    "                                         dtw_matrix[i, j-1],    # deletion\n",
    "                                         dtw_matrix[i-1, j-1])  # match\n",
    "    return np.exp(-(dtw_matrix[n, m]/((n+m)/2))*k)# change to *k,\n",
    "\n",
    "\n",
    "def create_distance_matrix(sentence_matrix,tau, k):\n",
    "    distance_matirx=np.zeros((60,4,4))\n",
    "    for _,i in enumerate(sentence_matrix):\n",
    "        for _1,each_talker1 in enumerate(i):\n",
    "            for _2,each_talker2 in enumerate(i):\n",
    "                distance_matirx[_][_1][_2]=dtw_sim(each_talker1, each_talker2, tau, k)\n",
    "    return distance_matirx\n",
    "#distance_matirx=create_distance_matrix(sentence_matrix1,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        SPA\n",
       "1        SPA\n",
       "2        SPA\n",
       "3        SPA\n",
       "4        SPA\n",
       "        ... \n",
       "10075    FAR\n",
       "10076    FAR\n",
       "10077    FAR\n",
       "10078    FAR\n",
       "10079    FAR\n",
       "Name: training_condition, Length: 10080, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_result[\"training_condition\"].str[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sim_measure(df, distance_matirx,tg_sentence_list,talkers):\n",
    "    out_df=pd.DataFrame(columns=['Condition', 'TrainingTalker', 'TestTalker','ListenerID' ,'Sentence',  'distance', 'score_test_logit','numCorrect','numWord'])\n",
    "    for each_ in tqdm.tqdm(df.values):\n",
    "        #distance_list=[]\n",
    "        \n",
    "        \n",
    "        if each_[1][0]==\"b\":\n",
    "            ind_T = talkers.index(each_[1][-3:])\n",
    "            arr = distance_matirx[tg_sentence_list.index(df.values[0][df.columns.get_loc(\"sentence_test\")])][ind_T]\n",
    "            ind_test=talkers.index(each_[-3])\n",
    "            \n",
    "            out_df.loc[len(out_df)]=[each_[df.columns.get_loc(\"st_mt_cnt\")],\n",
    "                                        each_[df.columns.get_loc(\"training_condition\")],\n",
    "                                        each_[df.columns.get_loc(\"speaker_test\")],\n",
    "                                        each_[df.columns.get_loc(\"id2\")],\n",
    "                                        each_[df.columns.get_loc(\"sentence_test\")],\n",
    "                                        arr[ind_test],\n",
    "                                        each_[df.columns.get_loc(\"score_test_logit\")],\n",
    "                                        each_[df.columns.get_loc(\"score_test\")],\n",
    "                                        each_[df.columns.get_loc(\"possible_score_test\")]\n",
    "                                        ]\n",
    "        else:\n",
    "            ind_T = talkers.index(each_[1][-3:])\n",
    "            ind_test=talkers.index(each_[-3])\n",
    "            arr = distance_matirx[tg_sentence_list.index(df.values[0][df.columns.get_loc(\"sentence_test\")])][ind_test]\n",
    "            indices=np.nonzero(arr)\n",
    "            arr_non_zero = arr[indices]\n",
    "            #print(each_[1],each_[-3],each_[1][-3:],each_[1][-3])\n",
    "            #break\n",
    "            if each_[1][-3:] == each_[-3]:\n",
    "                out_df.loc[len(out_df)]=[each_[df.columns.get_loc(\"st_mt_cnt\")],\n",
    "                                         each_[df.columns.get_loc(\"training_condition\")],\n",
    "                                         each_[df.columns.get_loc(\"speaker_test\")],\n",
    "                                         each_[df.columns.get_loc(\"id2\")],\n",
    "                                         each_[df.columns.get_loc(\"sentence_test\")],\n",
    "                                         np.mean(arr_non_zero),\n",
    "                                         each_[df.columns.get_loc(\"score_test_logit\")],\n",
    "                                         each_[df.columns.get_loc(\"score_test\")],\n",
    "                                         each_[df.columns.get_loc(\"possible_score_test\")]\n",
    "                                         ]\n",
    "    return out_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A BOY FELL FROM A WINDOW',\n",
       " 'BIG DOGS CAN BE DANGEROUS',\n",
       " 'THE SHOES WERE VERY DIRTY',\n",
       " 'THE PLAYER LOST A SHOE',\n",
       " \"SHE'S DRINKING FROM HER OWN CUP\",\n",
       " 'THE PICTURE CAME FROM A BOOK',\n",
       " 'THE CAR IS GOING TOO FAST',\n",
       " 'THE PAINT DRIPPED ON THE GROUND',\n",
       " 'THE TOWEL FELL ON THE FLOOR',\n",
       " 'THE KITCHEN WINDOW WAS CLEAN',\n",
       " 'THE MAILMAN BROUGHT A LETTER',\n",
       " 'THE MOTHER HEARD THE BABY',\n",
       " 'SHE FOUND HER PURSE IN THE TRASH',\n",
       " \"IT'S TIME TO GO TO BED\",\n",
       " 'MOTHER READ THE INSTRUCTIONS',\n",
       " 'THE DOG IS EATING SOME MEAT',\n",
       " 'THE PAINTER USES A BRUSH',\n",
       " 'SWIMMERS CAN HOLD THEIR BREATH',\n",
       " \"THEY'RE PUSHING AN OLD CAR\",\n",
       " 'THEY HAD TWO EMPTY BOTTLES',\n",
       " 'THE DOG SLEEPS IN A BASKET',\n",
       " \"THEY'RE PLAYING IN THE PARK\",\n",
       " 'THE BABY SLEPT ALL NIGHT',\n",
       " 'THE SALT SHAKER IS EMPTY',\n",
       " 'THE POLICEMAN KNOWS THE WAY',\n",
       " 'THE BUCKETS FILL UP QUICKLY',\n",
       " 'THE JANITOR SWEPT THE FLOOR',\n",
       " 'THE LADY WASHED THE SHIRT',\n",
       " 'THE MATCH BOXES ARE EMPTY',\n",
       " 'THE MAN IS PAINTING A SIGN',\n",
       " 'THE DOG CAME HOME AT LAST',\n",
       " 'THEY HEARD A FUNNY NOISE',\n",
       " 'THEY FOUND HIS BROTHER HIDING',\n",
       " 'THE DOG PLAYED WITH A STICK',\n",
       " 'THE BOOK TELLS A STORY',\n",
       " 'THE MATCHES ARE ON A SHELF',\n",
       " 'THE TEAM IS PLAYING WELL',\n",
       " 'THE SHIRTS ARE IN THE CLOSET',\n",
       " 'THEY WATCHED THE SCARY MOVIE',\n",
       " 'THE TALL MAN TIED HIS SHOES',\n",
       " 'A LETTER FELL ON THE FLOOR',\n",
       " 'THE BALL BOUNCED VERY HIGH',\n",
       " 'MOTHER CUT THE BIRTHDAY CAKE',\n",
       " 'THE FOOTBALL GAME IS OVER',\n",
       " 'SHE STOOD NEAR THE WINDOW',\n",
       " 'SHE USES HER SPOON TO EAT',\n",
       " 'THE CAT LAY ON THE BED',\n",
       " \"HE'S WASHING HIS FACE WITH SOAP\",\n",
       " 'THE DOG IS CHASING THE CAT',\n",
       " 'THE BABY HAS BLUE EYES',\n",
       " 'THE BAG FELL OFF THE SHELF',\n",
       " 'THEY KNOCKED ON THE WINDOW',\n",
       " 'THE FOOTBALL HIT THE GOALPOST',\n",
       " 'MOTHER GOT A SAUCEPAN',\n",
       " 'THE BABY WANTS HIS BOTTLE',\n",
       " 'THE BALL BROKE THE WINDOW',\n",
       " 'THE WAITER BROUGHT THE CREAM',\n",
       " 'THE GIRL IS WASHING HER HAIR',\n",
       " 'THE GIRL PLAYED WITH THE BABY',\n",
       " 'THEY ARE DRINKING COFFEE']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path=get_pathset(audio_dir)\n",
    "tg = textgrid.TextGrid.fromFile(audio_path[0][:-3]+\"TextGrid\")\n",
    "tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "hint1_list=get_sentence_ind(audio_path[0],human_result)\n",
    "tg_sentence=[tg_sentence[i].mark for i in hint1_list]\n",
    "tg1 = textgrid.TextGrid.fromFile(audio_path[1][:-3]+\"TextGrid\")\n",
    "tg_sentence1 = [i for i in tg1[0] if i.mark!=\"\"]\n",
    "hint2_list=get_sentence_ind(audio_path[1],human_result)\n",
    "tg_sentence1=[tg_sentence1[i].mark for i in hint2_list]\n",
    "tg_sentence_list=tg_sentence+tg_sentence1\n",
    "tg_sentence_list.index(human_result.values[0][human_result.columns.get_loc(\"sentence_test\")])\n",
    "\n",
    "tg_sentence_list\n",
    "\n",
    "#distance_matrix = create_distance_matrix(sentence_matrix1,tau)\n",
    "#distance_matrix = np.exp(-1*np.array(distance_matrix)**k)\n",
    "\n",
    "#out_df=sim_measure(human_result, distance_matrix, tg_sentence_list, talkers)\n",
    "#out_df[\"numIncorrect\"]=out_df[\"numWord\"]-out_df[\"numCorrect\"]\n",
    "#out_df[\"prop_correct\"]=out_df[\"numCorrect\"]/out_df[\"numWord\"]\n",
    "#out_df.to_csv(\"distances_df_T_min_tau=1.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PBR', 'FAR', 'TUR', 'SPA']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path=get_pathset(audio_dir)\n",
    "talkers=[os.path.basename(i)[10:13] for i in audio_path if \"HT1\" in i]\n",
    "talkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distance_matrix = create_distance_matrix(sentence_matrix1,tau)\n",
    "similarity_matrix = np.exp(-1*np.array(distance_matrix)*k)\n",
    "sim_measure(human_result, similarity_matrix,tg_sentence_list,talkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_mt = (distances1[\"Condition\"] == \"mt\") & (distances1[\"TrainingTalker\"].str[-3:] == distances1[\"TestTalker\"])\n",
    "mask_st = (distances1[\"Condition\"] == \"st \") & (distances1[\"TrainingTalker\"].str[-3:] != distances1[\"TestTalker\"])\n",
    "\n",
    "\n",
    "result = distances1[mask_mt | mask_st]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id2', 'training_condition', 'prop_correct_test',\n",
       "       'score_test', 'possible_score_test', 'sentence_test',\n",
       "       'sessiontype_test', 'group', 'speaker_test', 'score_test_logit',\n",
       "       'st_mt_cnt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ListenerID</th>\n",
       "      <th>TrainingTalker</th>\n",
       "      <th>prop_correct_test</th>\n",
       "      <th>numCorrect</th>\n",
       "      <th>numWord</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>sessiontype_test</th>\n",
       "      <th>group</th>\n",
       "      <th>TestTalker</th>\n",
       "      <th>score_test_logit</th>\n",
       "      <th>Condition</th>\n",
       "      <th>numIncorrect</th>\n",
       "      <th>prop_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>57e91ff08dcd2c00014bea18</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>THE BABY SLEPT ALL NIGHT</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>mt</td>\n",
       "      <td>1</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>128</td>\n",
       "      <td>57e91ff08dcd2c00014bea18</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>THE PAINT DRIPPED ON THE GROUND</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>mt</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>57e91ff08dcd2c00014bea18</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>SHE'S DRINKING FROM HER OWN CUP</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>mt</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>142</td>\n",
       "      <td>57e91ff08dcd2c00014bea18</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>THE PLAYER LOST A SHOE</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>mt</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>57e91ff08dcd2c00014bea18</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>THEY HAD TWO EMPTY BOTTLES</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>0.336472</td>\n",
       "      <td>mt</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>9826</td>\n",
       "      <td>62029b1eef1cb18a1337c797</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>SHE'S DRINKING FROM HER OWN CUP</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>mt</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9829</th>\n",
       "      <td>9829</td>\n",
       "      <td>62029b1eef1cb18a1337c797</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>A BOY FELL FROM A WINDOW</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>-0.587787</td>\n",
       "      <td>mt</td>\n",
       "      <td>4</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9831</th>\n",
       "      <td>9831</td>\n",
       "      <td>62029b1eef1cb18a1337c797</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>THEY HAD TWO EMPTY BOTTLES</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>mt</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9835</th>\n",
       "      <td>9835</td>\n",
       "      <td>62029b1eef1cb18a1337c797</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>MOTHER READ THE INSTRUCTIONS</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>mt</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9836</th>\n",
       "      <td>9836</td>\n",
       "      <td>62029b1eef1cb18a1337c797</td>\n",
       "      <td>c_noTUR</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>THE PLAYER LOST A SHOE</td>\n",
       "      <td>test</td>\n",
       "      <td>Test</td>\n",
       "      <td>TUR</td>\n",
       "      <td>-0.336472</td>\n",
       "      <td>mt</td>\n",
       "      <td>3</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1260 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                ListenerID TrainingTalker  prop_correct_test  \\\n",
       "121          121  57e91ff08dcd2c00014bea18        c_noTUR           0.800000   \n",
       "128          128  57e91ff08dcd2c00014bea18        c_noTUR           1.000000   \n",
       "135          135  57e91ff08dcd2c00014bea18        c_noTUR           1.000000   \n",
       "142          142  57e91ff08dcd2c00014bea18        c_noTUR           1.000000   \n",
       "144          144  57e91ff08dcd2c00014bea18        c_noTUR           0.600000   \n",
       "...          ...                       ...            ...                ...   \n",
       "9826        9826  62029b1eef1cb18a1337c797        c_noTUR           1.000000   \n",
       "9829        9829  62029b1eef1cb18a1337c797        c_noTUR           0.333333   \n",
       "9831        9831  62029b1eef1cb18a1337c797        c_noTUR           1.000000   \n",
       "9835        9835  62029b1eef1cb18a1337c797        c_noTUR           1.000000   \n",
       "9836        9836  62029b1eef1cb18a1337c797        c_noTUR           0.400000   \n",
       "\n",
       "      numCorrect  numWord                         Sentence sessiontype_test  \\\n",
       "121            4        5         THE BABY SLEPT ALL NIGHT             test   \n",
       "128            6        6  THE PAINT DRIPPED ON THE GROUND             test   \n",
       "135            6        6  SHE'S DRINKING FROM HER OWN CUP             test   \n",
       "142            5        5           THE PLAYER LOST A SHOE             test   \n",
       "144            3        5       THEY HAD TWO EMPTY BOTTLES             test   \n",
       "...          ...      ...                              ...              ...   \n",
       "9826           6        6  SHE'S DRINKING FROM HER OWN CUP             test   \n",
       "9829           2        6         A BOY FELL FROM A WINDOW             test   \n",
       "9831           5        5       THEY HAD TWO EMPTY BOTTLES             test   \n",
       "9835           4        4     MOTHER READ THE INSTRUCTIONS             test   \n",
       "9836           2        5           THE PLAYER LOST A SHOE             test   \n",
       "\n",
       "     group TestTalker  score_test_logit Condition  numIncorrect  prop_correct  \n",
       "121   Test        TUR          1.098612        mt             1      0.800000  \n",
       "128   Test        TUR          2.564949        mt             0      1.000000  \n",
       "135   Test        TUR          2.564949        mt             0      1.000000  \n",
       "142   Test        TUR          2.397895        mt             0      1.000000  \n",
       "144   Test        TUR          0.336472        mt             2      0.600000  \n",
       "...    ...        ...               ...       ...           ...           ...  \n",
       "9826  Test        TUR          2.564949        mt             0      1.000000  \n",
       "9829  Test        TUR         -0.587787        mt             4      0.333333  \n",
       "9831  Test        TUR          2.397895        mt             0      1.000000  \n",
       "9835  Test        TUR          2.197225        mt             0      1.000000  \n",
       "9836  Test        TUR         -0.336472        mt             3      0.400000  \n",
       "\n",
       "[1260 rows x 14 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = human_result\n",
    "out_df = out_df.rename(columns={\n",
    "        'st_mt_cnt': 'Condition',\n",
    "        'training_condition': 'TrainingTalker',\n",
    "        'speaker_test': 'TestTalker',\n",
    "        'id2': 'ListenerID',\n",
    "        'sentence_test': 'Sentence',\n",
    "        'score_test': 'numCorrect',\n",
    "        'possible_score_test': 'numWord'\n",
    "    })\n",
    "out_df['numIncorrect'] = out_df['numWord'] - out_df['numCorrect']\n",
    "out_df['prop_correct'] = out_df['numCorrect'] / out_df['numWord']\n",
    "training_suffix = out_df['TrainingTalker'].str[-3:]\n",
    "mask = ((out_df['Condition'] == 'st') & (training_suffix == out_df['TestTalker'])) | ((out_df['Condition'] == 'mt') & (training_suffix != out_df['TestTalker']))\n",
    "out_df = out_df[~mask]\n",
    "out_df[out_df[\"Condition\"]==\"mt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params, sentence_matrix1, human_result, tg_sentence_list, talkers):\n",
    "    tau, k = params\n",
    "    distance_matrix = create_distance_matrix(sentence_matrix1,tau)\n",
    "    similarity_matrix = np.exp(-1*np.array(distance_matrix)*k)\n",
    "    out_df=sim_measure(human_result, similarity_matrix,tg_sentence_list,talkers)\n",
    "    if np.std(out_df['similarity']) < 1.e-4:\n",
    "        return {'z_value': np.nan, 'mean_sim': np.mean(out_df['similarity']), 'sd_sim': np.std(out_df['similarity'])}\n",
    "    out_df = out_df.rename(columns={\n",
    "        'st_mt_cnt': 'Condition',\n",
    "        'training_condition': 'TrainingTalker',\n",
    "        'speaker_test': 'TestTalker',\n",
    "        'id2': 'ListenerID',\n",
    "        'sentence_test': 'Sentence',\n",
    "        'score_test': 'numCorrect',\n",
    "        'possible_score_test': 'numWord'\n",
    "    })\n",
    "    out_df['numIncorrect'] = out_df['numWord'] - out_df['numCorrect']\n",
    "    out_df['prop_correct'] = out_df['numCorrect'] / out_df['numWord']\n",
    "    \n",
    "    for col in ['TestTalker', 'Sentence', 'TrainingTalker', 'Condition']:\n",
    "        out_df[col] = out_df[col].astype('category')\n",
    "    \n",
    "    out_df['similarity_scaled'] = (out_df['similarity'] - np.mean(out_df['similarity'])) / (2 * np.std(out_df['similarity']))\n",
    "    \n",
    "    training_suffix = out_df['TrainingTalker'].str[-3:]\n",
    "    mask = ((out_df['Condition'] == 'st') & (training_suffix == out_df['TestTalker'])) | \\\n",
    "           ((out_df['Condition'] == 'mt') & (training_suffix != out_df['TestTalker']))\n",
    "    out_df = out_df[~mask]\n",
    "    if np.std(new_df['similarity']) < 0.3:\n",
    "        return {\n",
    "                    \"tau\": [tau], \n",
    "                    \"k\": [k], \n",
    "                    \"z-value\": [0],  \n",
    "                    \"mean_sim\": [float(np.mean(new_df2[\"similarity\"]))], \n",
    "                    \"sd_sim\": [float(np.std(new_df2[\"similarity\"]))],  \n",
    "                    \"error\": [\"std_sim<0.3\"]  # \n",
    "                }\n",
    "    else:\n",
    "        try:\n",
    "            import rpy2.robjects as ro\n",
    "            import rpy2\n",
    "            from rpy2.robjects import pandas2ri\n",
    "            \n",
    "            pandas2ri.activate()\n",
    "            base = importr('base')\n",
    "            stats = importr('stats')\n",
    "            lme4 = importr('lme4')\n",
    "            ro.r('options(warn=2)')\n",
    "            #r_data = ro.conversion.py2rpy(new_df2)\n",
    "            r_data = pandas2ri.py2rpy(new_df2)\n",
    "            formula = Formula('cbind(numCorrect, numIncorrect) ~ 1 + similarity + (1 | SentenceID / Keyword) + (1| TestTalkerID)')\n",
    "            glmerControl = lme4.glmerControl(optimizer=\"bobyqa\", optCtrl=ro.vectors.ListVector({'maxfun': 1e6}))\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                    \"tau\": [tau], \n",
    "                    \"k\": [k], \n",
    "                    \"z-value\": [0],  \n",
    "                    \"mean_sim\": [float(np.mean(new_df2[\"similarity\"]))], \n",
    "                    \"sd_sim\": [float(np.std(new_df2[\"similarity\"]))],  \n",
    "                    \"error\": [e]  # \n",
    "                }\n",
    "        try:\n",
    "            model = lme4.glmer(formula, data=r_data, control=glmerControl, family=stats.binomial(link=\"logit\"))\n",
    "            #log_likelihood = ro.r['logLik'](model)\n",
    "            summary = base.summary(model)\n",
    "            coefficients = summary.rx2('coefficients')\n",
    "            #z_value = coefficients.rx('similarity_scaled', 'z value')[0]\n",
    "            z_value=coefficients[1][2]\n",
    "            return {\n",
    "                    \"tau\": [tau], \n",
    "                    \"k\": [k], \n",
    "                    \"z-value\": [z_value],  \n",
    "                    \"mean_sim\": [float(np.mean(new_df2[\"similarity\"]))], \n",
    "                    \"sd_sim\": [float(np.std(new_df2[\"similarity\"]))],  \n",
    "                    \"error\": [\"NA\"]  # \n",
    "                }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                    \"tau\": [tau], \n",
    "                    \"k\": [k], \n",
    "                    \"z-value\": [0],  \n",
    "                    \"mean_sim\": [float(np.mean(new_df2[\"similarity\"]))], \n",
    "                    \"sd_sim\": [float(np.std(new_df2[\"similarity\"]))],  \n",
    "                    \"error\": [str(e)]  # \n",
    "                }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas2ri' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[1;32m----> 7\u001b[0m \u001b[43mpandas2ri\u001b[49m\u001b[38;5;241m.\u001b[39mactivate()\n\u001b[0;32m      8\u001b[0m params_grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mproduct(tau_values, k_values))\n\u001b[0;32m     12\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pandas2ri' is not defined"
     ]
    }
   ],
   "source": [
    "tau_values = np.arange(1,11,1)\n",
    "k_values = 10**np.arange(-4,1.1,0.1)\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "pandas2ri.activate()\n",
    "params_grid=list(itertools.product(tau_values, k_values))\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "n_jobs = min(30, multiprocessing.cpu_count())\n",
    "results = Parallel(n_jobs=n_jobs)(delayed(objective_function)(_, sentence_matrix1, human_result, tg_sentence_list, talkers) for _ in params_grid)\n",
    "end_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
